{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"200\" style=\"float:left\" \n",
    "     src=\"https://upload.wikimedia.org/wikipedia/commons/f/f3/Apache_Spark_logo.svg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float:left\" src=\"https://storage.googleapis.com/kaggle-competitions/kaggle/3136/logos/header.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sections\n",
    "* [Description](#0)\n",
    "* [1. Setup](#1)\n",
    "  * [1.1 Start Hadoop](#1.1)  \n",
    "  * [1.2 Search for Spark Installation](#1.2)\n",
    "  * [1.3 Create SparkSession](#1.3)\n",
    "* [2. Lab](#2)\n",
    "  * [2.1 Check Lab Files](#2.1)\n",
    "* [3. Data Preparation](#3)\n",
    "  * [3.1 Data Cleansing](#3.1)\n",
    "  * [3.1 Feature Engineering](#3.1)\n",
    "* [4. Model Training](#4)\n",
    "* [5. Model Evaluation](#5)\n",
    "* [6. Model Selection](#6)\n",
    "* [7. Model Persistence](#7)\n",
    "* [8. Model Loading](#8)\n",
    "* [9. Challenge](#9)\n",
    "* [10. TearDown](#10)\n",
    "  * [10.1 Stop Hadoop](#10.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='0'></a>\n",
    "## Description\n",
    "<p>\n",
    "<div>The goal for this lab is:</div>\n",
    "<ul>    \n",
    "    <li>Practice Spark's Machine Learning API</li>\n",
    "</ul>\n",
    "</p>\n",
    "\n",
    "The goal is to create a machine learning model to predict if a passenger would survive or not, therefore is a classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "## 1. Setup\n",
    "\n",
    "Since we are going to process data stored from HDFS let's start the service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1.1'></a>\n",
    "### 1.1 Start Hadoop\n",
    "\n",
    "Start Hadoop\n",
    "\n",
    "Open a terminal and execute\n",
    "```sh\n",
    "hadoop-start.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1.2'></a>\n",
    "### 1.2 Search for Spark Installation \n",
    "This step is only needed in our course environment; other Spark environments you might see out there, might not need this statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to improve how data is displayed, I'll setup Pandas accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1.3'></a>\n",
    "### 1.3 Create SparkSession\n",
    "\n",
    "By setting this environment variable we can include extra libraries in our Spark cluster.<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars /opt/hive3/lib/hive-hcatalog-core-3.1.2.jar pyspark-shell'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to create the SparkSession which we'll use to send our Spark code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "    .appName(\"Titanic - Analytics - MLlib\")\n",
    "    .config(\"spark.sql.warehouse.dir\",\"hdfs://localhost:9000/warehouse\")\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "## 2. Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2.1'></a>\n",
    "### 2.1 Check Lab Files\n",
    "\n",
    "In order to complete this lab you need to previosly upload the datasets into HDFS.<br/>\n",
    "\n",
    "Check you have the data ready in HDFS\n",
    "\n",
    "http://localhost:50070/explorer.html#/datalake/raw/kaggle/titanic/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2.2'></a>\n",
    "### 2.2 Data Information\n",
    "We have the following information about the dataset:\n",
    "<table style=\"float:left\">\n",
    "<tbody>\n",
    "<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\n",
    "<tr>\n",
    "<td>survival</td>\n",
    "<td>Survival</td>\n",
    "<td>0 = No, 1 = Yes</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>pclass</td>\n",
    "<td>Ticket class</td>\n",
    "<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>sex</td>\n",
    "<td>Sex</td>\n",
    "<td></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Age</td>\n",
    "<td>Age in years</td>\n",
    "<td></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>sibsp</td>\n",
    "<td># of siblings / spouses aboard the Titanic</td>\n",
    "<td></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>parch</td>\n",
    "<td># of parents / children aboard the Titanic</td>\n",
    "<td></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>ticket</td>\n",
    "<td>Ticket number</td>\n",
    "<td></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>fare</td>\n",
    "<td>Passenger fare</td>\n",
    "<td></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>cabin</td>\n",
    "<td>Cabin number</td>\n",
    "<td></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>embarked</td>\n",
    "<td>Port of Embarkation</td>\n",
    "<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "## 3. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3.1'></a>\n",
    "### 3.1 Data Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_raw = (spark.read\n",
    "                    .option(\"inferSchema\", \"true\")\n",
    "                    .option('header', 'true')\n",
    "                    .csv(\"hdfs://localhost:9000/datalake/raw/kaggle/titanic/\")\n",
    "                    .cache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_raw.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_raw.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some *Exploratory Data Analysis* to understand our data a bit better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passengers_count = titanic_raw.count()\n",
    "print (f\"Total number of passenger: {passengers_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary of data**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_raw.summary().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analize the number of **passengers who survived**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_raw.groupBy(\"Survived\").count().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "342 out of the 891 passengers survived."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's dig into **specific information about survivors** by exploring some more data.\n",
    "\n",
    "The survival rate can be determined by different features of the dataset such as *sex*, *port of embarcation*, *age*, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyze the **survival rate using feature sex**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_raw.groupBy(\"Sex\",\"Survived\").count().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the number of males is greater than the number of females in the ship, **females' survival rate is twice of the males'**.\n",
    "Let's analyze the **survival rate using feature pclass**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_raw.groupBy(\"Pclass\",\"Survived\").count().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's clear that people in *pclass #1* had much more priority than people in *pclass #3*; **even though the number of passengers in pclass #3 was higher, the survival rate was very low.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = titanic_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's **check if there are null values** that we need to remove before moving forward.\n",
    "\n",
    "There are **two ways of coming up with the number of null values**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnull, when, count, col\n",
    "\n",
    "# Option 1\n",
    "titanic_df.select([count(when(isnull(c), c)).alias(c) for c in titanic_df.columns]).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other way is summarizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2\n",
    "titanic_df.summary().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 features with missing data.\n",
    "\n",
    "**Cabin** feature has 687 (891 - 204) null values.\n",
    "\n",
    "**Embarked** feature has 2 (891 - 889) null values.\n",
    "\n",
    "**Age** feature has 177 (891 - 714) null values.\n",
    "\n",
    "Let's decide what to do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cabin\n",
    "Since there are so many missing values we have to get rid of this feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = titanic_df.drop(\"Cabin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embarked\n",
    "Embarked feature has only two missining values. Let's check values within Embarked\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.groupBy(\"Embarked\").count().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Majority Passengers boarded from \"S\". We can impute these with \"S\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = titanic_df.na.fill({\"Embarked\" : 'S'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Age\n",
    "This is going to be a tricky one :) in some scenarios you can replace a missing value with the mean,median, mode value of the dataset.There is a functionality available is Spark fot his job called <a href=\"https://spark.apache.org/docs/latest/ml-features.html#imputer\">Imputer</a>\n",
    "\n",
    "But this approach won't work here... **it might happen that you end up assignment a 4 years old kid a average age value of 29**.\n",
    "\n",
    "We'll try to come up with a representative number based on other features. The **Name feature seems to be a good one** to calculate an average age for missing values.\n",
    "\n",
    "**Names start with a word like Mr or Mrs**; calculating the average age for each group, will be closer to the real missing value... let's go for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_extract\n",
    "titanic_df = titanic_df.withColumn(\"Initial\",regexp_extract(col(\"Name\"),\"([A-Za-z]+)\\.\",1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regular expression \"([A-Za-z]+)\\\\.\" extracts the words we're looking for: **text containing characters between A-Z or a-z and followed by a .(dot)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.select(\"Initial\").distinct().sort(\"Initial\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some misspelled Initials like Mlle or Mme that stand for Miss. I will replace them with Miss and same thing for other values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = titanic_df.replace(\n",
    "               ['Mlle','Mme', 'Ms', 'Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don'],\n",
    "               ['Miss','Miss','Miss','Mr','Mr',  'Mrs',  'Mrs',  'Other',  'Other','Other','Mr','Mr','Mr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.select(\"Initial\").distinct().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's impute missing values in age feature based on average age of Initials\n",
    "1. Calculate the average value based on the initials and create a DataFrame with those values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import round\n",
    "\n",
    "avg_age_df = (titanic_df.groupby('Initial').avg('Age')\n",
    "                        .withColumnRenamed(\"avg(Age)\",\"Age\"))\n",
    "avg_age_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Let's create a temporary DF removing the Age column in those records/rows without a age (Age field with Null value):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df_noage = titanic_df.where(col(\"Age\").isNull()).drop(\"Age\")\n",
    "titanic_df_noage.limit(1).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Add an Age field to the previous DataFrame by joining it with the average age DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df_noage_with_avg = titanic_df_noage.join(avg_age_df, \"Initial\")\n",
    "titanic_df_noage_with_avg.limit(1).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Take the original titanic_df, **keep records/rows with a non-null age** and add the previous DataFrame to the result via an union transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df_fixed = (titanic_df.where(col(\"Age\").isNotNull())\n",
    "                      .unionByName(titanic_df_noage_with_avg))\n",
    "\n",
    "titanic_df_fixed.where(col(\"Age\").isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = titanic_df_fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Family_size and Alone\n",
    "Let's create a couple of features called **Family_size** and **Alone**, which might bring some insights on survival rate and the size of the families.\n",
    "\n",
    "**Family_size** is the total number of *parch (parents/children)* and *sibsp (siblings/spouses)* per row. **Alone** will be a flag set when the size of the family equals to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = titanic_df.withColumn(\"Family_Size\",col('SibSp')+col('Parch'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.groupBy(\"Family_Size\").count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "titanic_df = titanic_df.withColumn(\"Alone\",when(titanic_df[\"Family_Size\"] == 0, 1).otherwise(lit(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3.2'></a>\n",
    "### 3.2. Feature Engineering\n",
    "It's time to convert data into a suitable format for machine learning algorithms.<br/> \n",
    "First let's get rid of columns with unique values that don't contribute to a persons survival probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "titanic_df.select([countDistinct(c).alias(c) for c in titanic_df.columns]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = titanic_df.drop(\"PassengerId\",\"Name\",\"Ticket\",\"Initial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's cast all numerical values to doubles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = titanic_df.select(col('Survived').cast('double'),\n",
    "                              col('Pclass').cast('double'),\n",
    "                              col('Sex'),\n",
    "                              col('Age').cast('double'),\n",
    "                              col('SibSp').cast('double'),\n",
    "                              col('Parch').cast('double'),\n",
    "                              col('Fare').cast('double'),\n",
    "                              col('Embarked'),\n",
    "                              col('Family_Size').cast('double'),\n",
    "                              col('Alone').cast('double')\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Feature Transformation\n",
    "\n",
    "We need to **translate the values in string columns into  numerical values**.\n",
    "\n",
    "In order to do so, we are going to *encode* categorical values using:<br/>\n",
    "\n",
    "[StringIndexer](https://spark.apache.org/docs/latest/ml-features#stringindexer) <br/>\n",
    "[OneHotEncoder](https://spark.apache.org/docs/latest/ml-features#onehotencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_column = \"Survived\"\n",
    "\n",
    "categoricalCols = [field for (field, dataType) in titanic_df.dtypes if ((dataType == \"string\") & (field != label_column))]\n",
    "numericCols = [field for (field, dataType) in titanic_df.dtypes if ((dataType == \"double\") & (field != label_column))]\n",
    "\n",
    "print (f\"categorical columns: {categoricalCols}\")\n",
    "print (f\"numerical columns: {numericCols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexOutputCols = [x + \"Index\" for x in categoricalCols]\n",
    "oheOutputCols = [x + \"OHE\" for x in categoricalCols]\n",
    "\n",
    "print (f\"StringIndexer column names: {indexOutputCols}\")\n",
    "print (f\"OHE column names: {oheOutputCols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "stringIndexer = StringIndexer(inputCols=categoricalCols, outputCols=indexOutputCols, handleInvalid=\"skip\")\n",
    "\n",
    "oheEncoder = OneHotEncoder(inputCols=indexOutputCols,outputCols=oheOutputCols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = stringIndexer.fit(titanic_df).transform(titanic_df)\n",
    "temp_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oheEncoder.fit(temp_df).transform(temp_df).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assemblerInputs = oheOutputCols + numericCols\n",
    "print(\"Feature columns: \",assemblerInputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3 Feature Assembling\n",
    "\n",
    "It's finally time to **assemble the features in one single vector**, which is what the algorithm will expect, by using something called [VectorAssembler](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html?highlight=vectorassembler#pyspark.ml.feature.VectorAssembler).\n",
    "\n",
    "As the **\"Survived\" variable** is the one we want to predict, **all the other variables** will be considered to build the **list with required features**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vecAssembler = VectorAssembler(inputCols=assemblerInputs,outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "test_pipeline = Pipeline(stages = [stringIndexer, oheEncoder, vecAssembler])\n",
    "features_df = test_pipeline.fit(titanic_df).transform(titanic_df)\n",
    "features_df.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4'></a>\n",
    "## 4. Model Training\n",
    "\n",
    "Here is the list of few classification algorithms from Spark ML we are going to try:\n",
    "\n",
    "<ul>\n",
    "<li>LogisticRegression</li>\n",
    "<li>DecisionTreeClassifier</li>\n",
    "<li>RandomForestClassifier</li>\n",
    "<li>Gradient-boosted tree classifier</li>\n",
    "<li>NaiveBayes</li>\n",
    "<li>Linear Support Vector Machine</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some points in the machine learning workflow were randomness takes place, for example during the sets splitting and some ML algorithms like RandomForest.\n",
    "\n",
    "In order to make our experiments reproducible and get always the same results with the same data no matter how many times we execute our code, we are going to use a seed.\n",
    "\n",
    "The seed is tipically a prime number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "\n",
    "lr = LogisticRegression(labelCol=\"Survived\", featuresCol=\"features\")\n",
    "dt = DecisionTreeClassifier(labelCol=\"Survived\", featuresCol=\"features\",seed=seed)\n",
    "rf = RandomForestClassifier(labelCol=\"Survived\", featuresCol=\"features\",maxDepth=10,seed=seed)\n",
    "gbt = GBTClassifier(labelCol=\"Survived\", featuresCol=\"features\",maxIter=10,seed=seed)\n",
    "nb = NaiveBayes(labelCol=\"Survived\", featuresCol=\"features\")\n",
    "svm = LinearSVC(labelCol=\"Survived\", featuresCol=\"features\")\n",
    "\n",
    "classifiers = [lr,dt,rf,gbt,nb,svm]\n",
    "classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a pipeline for every classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "def create_pipeline(classifier):\n",
    "    return Pipeline(stages = [stringIndexer, oheEncoder, vecAssembler, classifier])\n",
    "\n",
    "pipelines = [create_pipeline(classifier) for classifier in classifiers]\n",
    "pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5'></a>\n",
    "## 5. Model Evaluation\n",
    "We're going to evaluate our classification model by using [MulticlassClassificationEvaluator](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.evaluation.MulticlassClassificationEvaluator.html)\n",
    "\n",
    "We're going to use the accuray metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Survived\",  metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6'></a>\n",
    "## 6. Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is all set, let's split it into training and test. We can use a 80-20 ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainingData, testData) = titanic_df.randomSplit([0.8,0.2],seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a good practice to keep the same distribution of 0's and 1's in the training set, and specially critical in umbalanced/skew datasets. This is called **Stratified Train-Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainingData1, testData1) = titanic_df.where(\"Survived=0\").randomSplit([0.8,0.2],seed=seed)\n",
    "(trainingData2, testData2) = titanic_df.where(\"Survived=1\").randomSplit([0.8,0.2],seed=seed)\n",
    "\n",
    "traininData = trainingData1.unionByName(trainingData2)\n",
    "testData = testData1.unionByName(testData2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train all the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [pipeline.fit(trainingData) for pipeline in pipelines]\n",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate all the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\n",
    "values = [] \n",
    "for model in models:\n",
    "    prediction_df = model.transform(testData)\n",
    "    accuracy = evaluator.evaluate(prediction_df)\n",
    "    names.append(type(model.stages[-1]).__name__) # the algorithm is the last stage in the pipeline\n",
    "    values.append(accuracy)\n",
    "\n",
    "data = {'name':names,'accuracy':values,'model':models}\n",
    "df = pd.DataFrame(data)\n",
    "df.sort_values(by=['accuracy'], inplace=True, ascending=False)  \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model is **RandomForestClassificationModel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model=df.iloc[0]['model']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.transform(testData).groupby(\"Survived\").pivot(\"prediction\").count().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='7'></a>\n",
    "## 7. Model Persistence\n",
    "Spark provides functionality to save the model/pipeline so that we can use it later for inference (batch or streaming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelPath = \"hdfs://localhost:9000/model-registry/titanic-survival-classifier\"\n",
    "best_model.write().overwrite().save(modelPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check directory contents\n",
    "\n",
    "http://localhost:50070/explorer.html#/model-registry/titanic-survival-classifier/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='8'></a>\n",
    "## 8. Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "savedModel = PipelineModel.load(modelPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = savedModel.transform(testData)\n",
    "predictions.select(\"features\", \"Survived\", \"prediction\").limit(200).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='9'></a>\n",
    "## 10. Challenge\n",
    "\n",
    "Â¿Can you improve this model?\n",
    "\n",
    "Try to find and remove outliers.\n",
    "\n",
    "Try new features or drop existing features.\n",
    "\n",
    "Try different feature transformations. \n",
    "\n",
    "Try different feature scalers.\n",
    "\n",
    "Try different algorithms and parameters.\n",
    "\n",
    "Try cross-validation or train-validation split with grid parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:white\">https://www.kaggle.com/startupsci/titanic-data-science-solutions</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='10'></a>\n",
    "## 10. Tear Down\n",
    "\n",
    "Once we complete the the lab we can stop all the services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='10.1'></a>\n",
    "### 10.1 Stop Hadoop\n",
    "\n",
    "Stops Hadoop\n",
    "Open a terminal and execute\n",
    "```sh\n",
    "hadoop-stop.sh\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
